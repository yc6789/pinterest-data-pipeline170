{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf10f4f-8bf9-4ab7-bf23-8d159f5b7ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee67d93-8288-40b9-bdc6-4fdbc034b9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd5416e-b000-4394-a236-946f02543128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the S3 bucket and mount point\n",
    "S3_BUCKET = \"user-0affd9571f39-bucket\"\n",
    "USER_ID = \"0affd9571f39\"\n",
    "\n",
    "# Define paths to JSON files\n",
    "pin_path = f\"s3a://{S3_BUCKET}/topics/{USER_ID}.pin/partition=0/\"\n",
    "geo_path = f\"s3a://{S3_BUCKET}/topics/{USER_ID}.geo/partition=0/\"\n",
    "user_path = f\"s3a://{S3_BUCKET}/topics/{USER_ID}.user/partition=0/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a615e918-9ad3-42c9-be78-e8fabec0b246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|      category|         description|downloaded|follower_count|           image_src|index|is_image_or_video|         poster_name|       save_location|            tag_list|               title|           unique_id|\n+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|     christmas|My favorite 75+ N...|         1|           31k|https://i.pinimg....| 2604|            image|  Everyday Wholesome|Local save in /da...|Colorful Christma...|75+ Neutral Chris...|087b0fa9-f901-426...|\n|        travel|See families trav...|         1|            9k|https://i.pinimg....|10119|            image|OUR NEXT ADVENTUR...|Local save in /da...|Family Vacation D...|How to Afford Fam...|40eab9ba-7812-4f2...|\n|diy-and-crafts|Easy Christmas tr...|         1|            3k|https://i.pinimg....| 3419|            image|Kids Crafts & Fre...|Local save in /da...|Christmas Crafts ...|Easy Christmas Tr...|d0b80187-0171-49b...|\n|       tattoos|\"Minimalist Tatto...|         1|           54k|https://i.pinimg....| 9093|            image|Beautyholo | Late...|Local save in /da...|Small Tattoos Men...|51 Unique Minimal...|8e478adb-0e3f-404...|\n|diy-and-crafts|Celebrate warmer ...|         1|          108k|https://i.pinimg....| 2858|            image|This Tiny  Blue H...|Local save in /da...|Spring Crafts For...|50 Popular Spring...|26f81da4-26a9-465...|\n|        quotes|Trying to create ...|         1|           42k|https://i.pinimg....| 8312|            image|TheFab20s | Trave...|Local save in /da...|Positive Self Aff...|8 Vision Board Id...|ca3c9bb0-7281-4b9...|\n|           art|Use your mini wor...|         1|            4k|https://i.pinimg....|  771|            image|Taming Little Mon...|Local save in /da...|African Art Proje...|African Sunset Sh...|a5021766-a8aa-4dc...|\n|     education|Hi everyone! As a...|         1|           22k|https://i.pinimg....| 4076|            image|   The Literacy Nest|Local save in /da...|Literacy Games,Ki...|Phonics Activitie...|3a52d364-7c04-47c...|\n|     christmas|Over 40 of the BE...|         1|          245k|https://i.pinimg....| 2293|            image|Kitchen Fun With ...|Local save in /da...|Diy Christmas Lig...|Over 40 of the BE...|1fd7d4cc-54c1-454...|\n|           art|This bee directed...|         1|            1M|https://i.pinimg....|  159|            image|Teachers Pay Teac...|Local save in /da...|Classroom Art Pro...|Valentine's Day B...|841a161a-47b8-416...|\n|           art|Easy to follow st...|         1|           52k|https://i.pinimg....|  427|            video|     Abbotts At Home|Local save in /da...|Acrylic Pouring A...|DIY Acrylic Paint...|4a455340-09a2-437...|\n|     education|This book present...|         1|            2M|https://i.pinimg....| 3599|            image|             Walmart|Local save in /da...|Research Studies,...|Educational Leade...|ff0dd945-dafa-411...|\n|       tattoos|Disney characters...|         1|          190k|https://i.pinimg....| 9074|            image|    Our Mindful Life|Local save in /da...|Small Disney Tatt...|77 Disney Tattoos...|2adede06-7fb0-4b5...|\n|        quotes|Who needs anyone ...|         1|          942k|https://i.pinimg....| 7874|            image|           YourTango|Local save in /da...|Good Quotes,Motiv...|35 Happiness Quot...|5343b4fb-36a8-4f3...|\n|  mens-fashion|Hamilton Khaki Av...|         1|           265|https://i.pinimg....| 7167|            image|Midwest Jewelers ...|Local save in /da...|Swiss Army Watche...|Hamilton Khaki Av...|922425d2-16dc-465...|\n|           art|Nike trainers spr...|         1|          908k|https://i.pinimg....|  552|            image|        The Guardian|Local save in /da...|Art And Illustrat...|The herbal bed: K...|b82602a9-e434-42c...|\n|       tattoos|Snake temporary t...|         1|             0|https://i.pinimg....| 8578|            image|       Livitrevisani|Local save in /da...|Finger Tattoo Des...|Snake Collarbone ...|036ec267-658b-4da...|\n|        travel|From practical tr...|         1|           41k|https://i.pinimg....|10245|            image|Her Packing List ...|Local save in /da...|Travel To Do,Trav...|99 Things Every F...|cb3aee98-8171-44c...|\n|        beauty|It can be used as...|         1|            5M|https://i.pinimg....| 1450|            image|            BuzzFeed|Local save in /da...|Face Care,Body Ca...|A multitasking cl...|be11eb0e-bf1d-455...|\n|    home-decor|Celebrate fall wi...|         1|          178k|https://i.pinimg....| 6566|            image|      Sand and Sisal|Local save in /da...|Fall Living Room,...|Navy and Neutral ...|6d8848f8-f515-493...|\n+--------------+--------------------+----------+--------------+--------------------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n+--------------------+-----+--------+---------+-------------------+\n|             country|  ind|latitude|longitude|          timestamp|\n+--------------------+-----+--------+---------+-------------------+\n|Saint Vincent and...| 2301| 13.4683|  51.7244|2020-11-14T00:25:28|\n|British Virgin Is...| 2293|-87.7946| -159.647|2022-03-21T10:46:53|\n| Antigua and Barbuda| 2604|-80.8933| -104.972|2018-12-01T09:23:35|\n|Netherlands Antilles|  603| 14.0083| -141.603|2019-06-25T05:13:01|\n|    Christmas Island|10119|-74.5431| -162.795|2020-10-22T01:59:58|\n| Trinidad and Tobago| 2060| 52.4584|  68.6527|2020-01-25T13:54:17|\n|    Marshall Islands| 6261|-10.3101| -109.763|2021-11-11T16:18:45|\n|    Pitcairn Islands| 3201| 34.0532| -68.4946|2019-06-25T08:31:37|\n|      American Samoa| 1422|-88.5252| -172.436|2018-04-30T08:27:21|\n|      American Samoa| 8312|-77.9744| -106.258|2021-04-25T15:56:29|\n|      American Samoa| 1450|-88.5252| -172.436|2022-01-15T16:04:27|\n|       Guinea-Bissau| 8578| 34.6427| -161.397|2018-01-26T19:27:11|\n|       New Caledonia| 6844|-22.6915|  5.69245|2021-06-06T21:53:11|\n|         Afghanistan| 3599|-88.5478| -174.971|2019-03-03T06:13:41|\n|          Madagascar| 4988|-32.1879| -35.2784|2020-07-11T11:01:12|\n|         Isle of Man|  427|-66.9418| -30.0087|2020-04-22T03:08:50|\n|          Azerbaijan| 5758|-79.3714| -145.242|2018-09-12T15:08:34|\n|           Argentina| 8930|-89.4739| -176.154|2021-09-29T13:25:49|\n|           Australia| 7294|-76.2967| -136.501|2018-02-12T08:06:28|\n|          Montserrat|  771|-29.1712| -107.111|2018-06-21T08:42:57|\n+--------------------+-----+--------+---------+-------------------+\nonly showing top 20 rows\n\n+---+-------------------+-----------+-----+---------+\n|age|        date_joined| first_name|  ind|last_name|\n+---+-------------------+-----------+-----+---------+\n| 27|2016-03-08T13:38:37|Christopher| 2015| Bradshaw|\n| 59|2017-05-12T21:22:17|  Alexander|10673|Cervantes|\n| 39|2016-06-29T20:43:59|  Christina| 6398|Davenport|\n| 20|2015-10-23T04:13:23| Alexandria| 3599| Alvarado|\n| 49|2016-04-22T20:36:02|   Brittany|10509| Thompson|\n| 43|2016-07-21T15:25:08|    Chelsea|10119| Gonzalez|\n| 23|2015-12-01T18:15:02|  Christine| 7768|   Cortez|\n| 23|2015-11-28T11:52:37|     Andrew| 8930| Anderson|\n| 31|2017-08-04T14:30:22|  Alexander| 6566|    Perez|\n| 52|2016-02-07T20:00:25|    Richard| 3729|  Edwards|\n| 40|2017-05-16T07:09:21|    Michael|10552|   Hunter|\n| 21|2015-10-25T07:36:08|      Aaron| 9074|Alexander|\n| 28|2016-01-29T20:32:05|  Elizabeth| 3201|    Terry|\n| 32|2016-03-10T04:11:31|   Brittany|  771|   Butler|\n| 46|2015-11-27T23:11:21|   Jonathan| 1545|    Avila|\n| 44|2016-10-29T02:07:21|    Brandon|  603|  Jackson|\n| 22|2016-02-09T17:01:38|     Carlos| 5630|  Estrada|\n| 32|2016-06-08T22:10:13|      Donna| 1268| Campbell|\n| 54|2016-05-15T04:22:01|     Alexis| 1555|  Bennett|\n| 20|2015-12-17T08:43:40|       Adam| 3800|Armstrong|\n+---+-------------------+-----------+-----+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Read Pinterest post data\n",
    "df_pin = spark.read.json(pin_path)\n",
    "df_pin.show()  # Display the Pinterest post data\n",
    "\n",
    "# Read Geolocation data\n",
    "df_geo = spark.read.json(geo_path)\n",
    "df_geo.show()  # Display the Geolocation data\n",
    "\n",
    "# Read User data\n",
    "df_user = spark.read.json(user_path)\n",
    "df_user.show()  # Display the User data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6a67b4-27a8-416f-9155-704caf221e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------+\n|  ind|           unique_id|               title|         description|follower_count|         poster_name|            tag_list|is_image_or_video|           image_src|       save_location|      category|\n+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------+\n| 2604|087b0fa9-f901-426...|75+ Neutral Chris...|My favorite 75+ N...|         31000|  Everyday Wholesome|Colorful Christma...|            image|https://i.pinimg....|Local save in /da...|     christmas|\n|10119|40eab9ba-7812-4f2...|How to Afford Fam...|See families trav...|          9000|OUR NEXT ADVENTUR...|Family Vacation D...|            image|https://i.pinimg....|Local save in /da...|        travel|\n| 3419|d0b80187-0171-49b...|Easy Christmas Tr...|Easy Christmas tr...|          3000|Kids Crafts & Fre...|Christmas Crafts ...|            image|https://i.pinimg....|Local save in /da...|diy-and-crafts|\n| 9093|8e478adb-0e3f-404...|51 Unique Minimal...|\"Minimalist Tatto...|         54000|Beautyholo | Late...|Small Tattoos Men...|            image|https://i.pinimg....|Local save in /da...|       tattoos|\n| 2858|26f81da4-26a9-465...|50 Popular Spring...|Celebrate warmer ...|        108000|This Tiny  Blue H...|Spring Crafts For...|            image|https://i.pinimg....|Local save in /da...|diy-and-crafts|\n| 8312|ca3c9bb0-7281-4b9...|8 Vision Board Id...|Trying to create ...|         42000|TheFab20s | Trave...|Positive Self Aff...|            image|https://i.pinimg....|Local save in /da...|        quotes|\n|  771|a5021766-a8aa-4dc...|African Sunset Sh...|Use your mini wor...|          4000|Taming Little Mon...|African Art Proje...|            image|https://i.pinimg....|Local save in /da...|           art|\n| 4076|3a52d364-7c04-47c...|Phonics Activitie...|Hi everyone! As a...|         22000|   The Literacy Nest|Literacy Games,Ki...|            image|https://i.pinimg....|Local save in /da...|     education|\n| 2293|1fd7d4cc-54c1-454...|Over 40 of the BE...|Over 40 of the BE...|        245000|Kitchen Fun With ...|Diy Christmas Lig...|            image|https://i.pinimg....|Local save in /da...|     christmas|\n|  159|841a161a-47b8-416...|Valentine's Day B...|This bee directed...|       1000000|Teachers Pay Teac...|Classroom Art Pro...|            image|https://i.pinimg....|Local save in /da...|           art|\n|  427|4a455340-09a2-437...|DIY Acrylic Paint...|Easy to follow st...|         52000|     Abbotts At Home|Acrylic Pouring A...|            video|https://i.pinimg....|Local save in /da...|           art|\n| 3599|ff0dd945-dafa-411...|Educational Leade...|This book present...|       2000000|             Walmart|Research Studies,...|            image|https://i.pinimg....|Local save in /da...|     education|\n| 9074|2adede06-7fb0-4b5...|77 Disney Tattoos...|Disney characters...|        190000|    Our Mindful Life|Small Disney Tatt...|            image|https://i.pinimg....|Local save in /da...|       tattoos|\n| 7874|5343b4fb-36a8-4f3...|35 Happiness Quot...|Who needs anyone ...|        942000|           YourTango|Good Quotes,Motiv...|            image|https://i.pinimg....|Local save in /da...|        quotes|\n| 7167|922425d2-16dc-465...|Hamilton Khaki Av...|Hamilton Khaki Av...|           265|Midwest Jewelers ...|Swiss Army Watche...|            image|https://i.pinimg....|Local save in /da...|  mens-fashion|\n|  552|b82602a9-e434-42c...|The herbal bed: K...|Nike trainers spr...|        908000|        The Guardian|Art And Illustrat...|            image|https://i.pinimg....|Local save in /da...|           art|\n| 8578|036ec267-658b-4da...|Snake Collarbone ...|Snake temporary t...|             0|       Livitrevisani|Finger Tattoo Des...|            image|https://i.pinimg....|Local save in /da...|       tattoos|\n|10245|cb3aee98-8171-44c...|99 Things Every F...|From practical tr...|         41000|Her Packing List ...|Travel To Do,Trav...|            image|https://i.pinimg....|Local save in /da...|        travel|\n| 1450|be11eb0e-bf1d-455...|A multitasking cl...|It can be used as...|       5000000|            BuzzFeed|Face Care,Body Ca...|            image|https://i.pinimg....|Local save in /da...|        beauty|\n| 6566|6d8848f8-f515-493...|Navy and Neutral ...|Celebrate fall wi...|        178000|      Sand and Sisal|Fall Living Room,...|            image|https://i.pinimg....|Local save in /da...|    home-decor|\n+-----+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+-----------------+--------------------+--------------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, regexp_replace, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1. Replace empty entries and entries with no relevant data with None\n",
    "df_pin_cleaned = df_pin.replace([\"\", \" \", \"null\", \"N/A\"], None)\n",
    "\n",
    "# 2. Transform `follower_count` to ensure every entry is a number\n",
    "# Handle cases where follower_count contains strings like \"1k\" or \"1M\"\n",
    "def parse_follower_count(follower_count):\n",
    "    if follower_count is None:\n",
    "        return None\n",
    "    if isinstance(follower_count, str):\n",
    "        if \"k\" in follower_count.lower():\n",
    "            return int(float(follower_count.lower().replace(\"k\", \"\")) * 1000)\n",
    "        elif \"m\" in follower_count.lower():\n",
    "            return int(float(follower_count.lower().replace(\"m\", \"\")) * 1000000)\n",
    "    try:\n",
    "        return int(follower_count)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "parse_follower_count_udf = udf(parse_follower_count, IntegerType())\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"follower_count\", parse_follower_count_udf(col(\"follower_count\"))\n",
    ")\n",
    "\n",
    "# 3. Ensure numeric columns have numeric data types\n",
    "numeric_columns = [\"follower_count\"]  # Add other numeric columns if necessary\n",
    "for column in numeric_columns:\n",
    "    df_pin_cleaned = df_pin_cleaned.withColumn(column, col(column).cast(\"int\"))\n",
    "\n",
    "# 4. Clean `save_location` to include only the save location path\n",
    "# Assuming `save_location` has some unwanted prefixes or suffixes to clean\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"save_location\", regexp_replace(col(\"save_location\"), r\"unwanted_pattern\", \"\")\n",
    ")\n",
    "\n",
    "# 5. Rename the `index` column to `ind`\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# 6. Reorder columns to the desired order\n",
    "desired_column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\",\n",
    "]\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.select(*desired_column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_pin_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00082cbe-6895-44c3-a0eb-3ed7aeb4cc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------------------+\n|  ind|             country|         coordinates|          timestamp|\n+-----+--------------------+--------------------+-------------------+\n| 2301|Saint Vincent and...|  [13.4683, 51.7244]|2020-11-14 00:25:28|\n| 2293|British Virgin Is...|[-87.7946, -159.647]|2022-03-21 10:46:53|\n| 2604| Antigua and Barbuda|[-80.8933, -104.972]|2018-12-01 09:23:35|\n|  603|Netherlands Antilles| [14.0083, -141.603]|2019-06-25 05:13:01|\n|10119|    Christmas Island|[-74.5431, -162.795]|2020-10-22 01:59:58|\n| 2060| Trinidad and Tobago|  [52.4584, 68.6527]|2020-01-25 13:54:17|\n| 6261|    Marshall Islands|[-10.3101, -109.763]|2021-11-11 16:18:45|\n| 3201|    Pitcairn Islands| [34.0532, -68.4946]|2019-06-25 08:31:37|\n| 1422|      American Samoa|[-88.5252, -172.436]|2018-04-30 08:27:21|\n| 8312|      American Samoa|[-77.9744, -106.258]|2021-04-25 15:56:29|\n| 1450|      American Samoa|[-88.5252, -172.436]|2022-01-15 16:04:27|\n| 8578|       Guinea-Bissau| [34.6427, -161.397]|2018-01-26 19:27:11|\n| 6844|       New Caledonia| [-22.6915, 5.69245]|2021-06-06 21:53:11|\n| 3599|         Afghanistan|[-88.5478, -174.971]|2019-03-03 06:13:41|\n| 4988|          Madagascar|[-32.1879, -35.2784]|2020-07-11 11:01:12|\n|  427|         Isle of Man|[-66.9418, -30.0087]|2020-04-22 03:08:50|\n| 5758|          Azerbaijan|[-79.3714, -145.242]|2018-09-12 15:08:34|\n| 8930|           Argentina|[-89.4739, -176.154]|2021-09-29 13:25:49|\n| 7294|           Australia|[-76.2967, -136.501]|2018-02-12 08:06:28|\n|  771|          Montserrat|[-29.1712, -107.111]|2018-06-21 08:42:57|\n+-----+--------------------+--------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, to_timestamp\n",
    "\n",
    "# 1. Create a new column `coordinates` that contains an array based on the latitude and longitude columns\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "\n",
    "# 2. Drop the `latitude` and `longitude` columns\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# 3. Convert the `timestamp` column from a string to a timestamp data type\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# 4. Reorder the DataFrame columns to have the desired column order\n",
    "desired_column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo_cleaned = df_geo_cleaned.select(*desired_column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_geo_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4b0215-736f-4b16-983a-c33a2c7cd210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+-------------------+\n|  ind|           user_name|age|        date_joined|\n+-----+--------------------+---+-------------------+\n| 2015|Christopher Bradshaw| 27|2016-03-08 13:38:37|\n|10673| Alexander Cervantes| 59|2017-05-12 21:22:17|\n| 6398| Christina Davenport| 39|2016-06-29 20:43:59|\n| 3599| Alexandria Alvarado| 20|2015-10-23 04:13:23|\n|10509|   Brittany Thompson| 49|2016-04-22 20:36:02|\n|10119|    Chelsea Gonzalez| 43|2016-07-21 15:25:08|\n| 7768|    Christine Cortez| 23|2015-12-01 18:15:02|\n| 8930|     Andrew Anderson| 23|2015-11-28 11:52:37|\n| 6566|     Alexander Perez| 31|2017-08-04 14:30:22|\n| 3729|     Richard Edwards| 52|2016-02-07 20:00:25|\n|10552|      Michael Hunter| 40|2017-05-16 07:09:21|\n| 9074|     Aaron Alexander| 21|2015-10-25 07:36:08|\n| 3201|     Elizabeth Terry| 28|2016-01-29 20:32:05|\n|  771|     Brittany Butler| 32|2016-03-10 04:11:31|\n| 1545|      Jonathan Avila| 46|2015-11-27 23:11:21|\n|  603|     Brandon Jackson| 44|2016-10-29 02:07:21|\n| 5630|      Carlos Estrada| 22|2016-02-09 17:01:38|\n| 1268|      Donna Campbell| 32|2016-06-08 22:10:13|\n| 1555|      Alexis Bennett| 54|2016-05-15 04:22:01|\n| 3800|      Adam Armstrong| 20|2015-12-17 08:43:40|\n+-----+--------------------+---+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, to_timestamp\n",
    "\n",
    "# 1. Create a new column `user_name` by concatenating `first_name` and `last_name`\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "\n",
    "# 2. Drop the `first_name` and `last_name` columns\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# 3. Convert the `date_joined` column from a string to a timestamp data type\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "# 4. Reorder the DataFrame columns to have the desired column order\n",
    "desired_column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user_cleaned = df_user_cleaned.select(*desired_column_order)\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_user_cleaned.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6329765c-5b6c-47ef-b6cb-1522533496a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------+\n|            country|      category|category_count|\n+-------------------+--------------+--------------+\n|        Afghanistan|     education|             1|\n|        Afghanistan|           art|             1|\n|            Albania|           art|             1|\n|            Albania|    home-decor|             1|\n|            Algeria|        quotes|             2|\n|     American Samoa|        beauty|             2|\n|            Andorra|       tattoos|             1|\n|            Andorra|           art|             1|\n|            Andorra|        beauty|             1|\n|           Anguilla|        beauty|             1|\n|Antigua and Barbuda|     christmas|             1|\n|          Argentina|diy-and-crafts|             1|\n|          Argentina|        beauty|             1|\n|          Argentina|       tattoos|             1|\n|            Armenia|     christmas|             1|\n|            Armenia|        beauty|             1|\n|              Aruba|  mens-fashion|             1|\n|              Aruba|     education|             1|\n|          Australia|        travel|             1|\n|          Australia|  mens-fashion|             1|\n+-------------------+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, count, rank\n",
    "\n",
    "# Join df_pin and df_geo on 'ind'\n",
    "df_combined = df_pin_cleaned.join(df_geo_cleaned, on=\"ind\", how=\"inner\")\n",
    "\n",
    "# Group by country and category, and count the number of posts\n",
    "category_counts = df_combined.groupBy(\"country\", \"category\").agg(\n",
    "    count(\"*\").alias(\"category_count\")\n",
    ")\n",
    "\n",
    "# Define a window partitioned by country and ordered by category_count in descending order\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Add a rank column to identify the top category per country\n",
    "ranked_categories = category_counts.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter to only include the most popular category (rank = 1) per country\n",
    "most_popular_categories = ranked_categories.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show the results\n",
    "most_popular_categories.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2369bb2e-2b8a-472c-a05a-803024680818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n|post_year|      category|category_count|\n+---------+--------------+--------------+\n|     2018|           art|             3|\n|     2018|        beauty|             3|\n|     2018|     christmas|             2|\n|     2018|diy-and-crafts|             1|\n|     2018|     education|             2|\n|     2018|       finance|             1|\n|     2018|  mens-fashion|             1|\n|     2018|        quotes|             1|\n|     2018|       tattoos|             1|\n|     2018|        travel|             2|\n|     2018|      vehicles|             1|\n|     2019|           art|             2|\n|     2019|        beauty|             1|\n|     2019|diy-and-crafts|             2|\n|     2019|     education|             2|\n|     2019|       finance|             1|\n|     2019|  mens-fashion|             1|\n|     2019|      vehicles|             1|\n|     2020|           art|             1|\n|     2020|        beauty|             2|\n+---------+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col, count\n",
    "\n",
    "# Extract the year from the timestamp column\n",
    "df_pin_with_year = df_combined.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Filter posts between 2018 and 2022\n",
    "df_filtered = df_pin_with_year.filter((col(\"post_year\") >= 2018) & (col(\"post_year\") <= 2022))\n",
    "\n",
    "# Group by post_year and category, and count the number of posts\n",
    "df_category_count = df_filtered.groupBy(\"post_year\", \"category\").agg(\n",
    "    count(\"*\").alias(\"category_count\")\n",
    ")\n",
    "\n",
    "# Order the results for better readability\n",
    "df_category_count = df_category_count.orderBy(\"post_year\", \"category\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_category_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843c74c1-4b93-43a7-a75a-9fd3eafb2a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|       country|follower_count|\n+--------------+--------------+\n|American Samoa|       5000000|\n|American Samoa|       5000000|\n+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Select the required columns\n",
    "df_query_step1 = df_combined.select(\"country\", \"poster_name\", \"follower_count\")\n",
    "\n",
    "# Define a window partitioned by country and ordered by follower_count in descending order\n",
    "window_spec = Window.orderBy(col(\"follower_count\").desc())\n",
    "\n",
    "# Rank users globally based on follower_count\n",
    "df_ranked = df_query_step1.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter the top-ranked user\n",
    "df_top_follower = df_ranked.filter(col(\"rank\") == 1).select(\"country\", \"follower_count\")\n",
    "\n",
    "# Show the result\n",
    "df_top_follower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a3f22e-7a03-421a-90d7-25c66dadbc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------+\n|age_group|      category|category_count|\n+---------+--------------+--------------+\n|      +50|        beauty|             2|\n|    18-24|        beauty|             4|\n|    18-24|  mens-fashion|             4|\n|    25-35|           art|             4|\n|    25-35|     christmas|             4|\n|    36-50|    home-decor|             2|\n|    36-50|        beauty|             2|\n|    36-50|      vehicles|             2|\n|    36-50|diy-and-crafts|             2|\n|    36-50|           art|             2|\n+---------+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_user_pin = df_pin_cleaned.join(df_user_cleaned,how=\"inner\",on=\"ind\")\n",
    "# Step 1: Create an age_group column\n",
    "df_with_age_group = df_user_pin.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .when(col(\"age\") > 50, \"+50\")\n",
    "    .otherwise(None)  # Exclude users with age outside these ranges\n",
    ")\n",
    "\n",
    "# Step 2: Group by age_group and category, and count the number of posts\n",
    "category_counts = df_with_age_group.groupBy(\"age_group\", \"category\").agg(\n",
    "    count(\"*\").alias(\"category_count\")\n",
    ")\n",
    "\n",
    "# Step 3: Rank categories within each age group\n",
    "window_spec = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "ranked_categories = category_counts.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Step 4: Filter for the most popular category in each age group\n",
    "most_popular_categories = ranked_categories.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Show the result\n",
    "most_popular_categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d36108-430d-4321-a267-78e55fb56c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n|post_year|number_users_joined|\n+---------+-------------------+\n|     2015|                 20|\n|     2016|                 32|\n|     2017|                  8|\n+---------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col, count\n",
    "\n",
    "# Step 1: Extract the year from the date_joined column\n",
    "df_user_with_year = df_user.withColumn(\"post_year\", year(\"date_joined\"))\n",
    "\n",
    "# Step 2: Filter rows where post_year is between 2015 and 2020\n",
    "df_filtered = df_user_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Step 3: Group by post_year and count the number of users\n",
    "df_users_joined = df_filtered.groupBy(\"post_year\").agg(\n",
    "    count(\"*\").alias(\"number_users_joined\")\n",
    ")\n",
    "\n",
    "# Step 4: Order the results by post_year\n",
    "df_users_joined = df_users_joined.orderBy(\"post_year\")\n",
    "\n",
    "# Show the results\n",
    "df_users_joined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00585233-02af-4942-b79d-af3160b43244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+\n|post_year|median_follower_count|\n+---------+---------------------+\n|     2017|             613000.0|\n|     2018|              22000.0|\n|     2019|              22000.0|\n|     2020|               9000.0|\n+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the year from the timestamp column\n",
    "df_combined_with_year = df_combined.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Step 2: Filter rows where post_year is between 2015 and 2020\n",
    "df_filtered = df_combined_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Step 3: Calculate the median follower count for each year\n",
    "results = []\n",
    "post_years = df_filtered.select(\"post_year\").distinct().collect()  # Get unique years\n",
    "\n",
    "for row in post_years:\n",
    "    post_year = row[\"post_year\"]\n",
    "    \n",
    "    # Filter data for the specific year\n",
    "    df_year = df_filtered.filter(col(\"post_year\") == post_year)\n",
    "    \n",
    "    # Calculate the median using approxQuantile\n",
    "    median_follower_count = df_year.approxQuantile(\"follower_count\", [0.5], 0.01)[0]\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results.append((post_year, median_follower_count))\n",
    "\n",
    "# Step 4: Create a new DataFrame for the results\n",
    "schema = StructType([\n",
    "    StructField(\"post_year\", IntegerType(), True),\n",
    "    StructField(\"median_follower_count\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_median_follower_count = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Order the DataFrame by post_year\n",
    "df_median_follower_count = df_median_follower_count.orderBy(\"post_year\")\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae314d4-d687-4a7e-82f9-415fec8b2a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------------+\n|age_group|post_year|median_follower_count|\n+---------+---------+---------------------+\n|      +50|     2017|             326000.0|\n|      +50|     2018|               9000.0|\n|      +50|     2020|                 20.0|\n|    18-24|     2017|             613000.0|\n|    18-24|     2018|              28000.0|\n|    18-24|     2019|              22000.0|\n|    25-35|     2018|              22000.0|\n|    25-35|     2019|               3000.0|\n|    25-35|     2020|              52000.0|\n|    36-50|     2018|                  0.0|\n|    36-50|     2019|              89000.0|\n|    36-50|     2020|                314.0|\n+---------+---------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_combined_with_user = df_combined_with_year.join(df_user_cleaned,how=\"inner\",on=\"ind\")\n",
    "\n",
    "# Step 1: Create an age_group column\n",
    "df_with_age_group = df_combined_with_user.withColumn(\n",
    "    \"age_group\",\n",
    "    when((col(\"age\") >= 18) & (col(\"age\") <= 24), \"18-24\")\n",
    "    .when((col(\"age\") >= 25) & (col(\"age\") <= 35), \"25-35\")\n",
    "    .when((col(\"age\") >= 36) & (col(\"age\") <= 50), \"36-50\")\n",
    "    .when(col(\"age\") > 50, \"+50\")\n",
    "    .otherwise(None)  # Exclude rows with invalid age\n",
    ")\n",
    "\n",
    "# Step 2: Extract post_year from timestamp\n",
    "df_with_year = df_with_age_group.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Step 3: Filter rows where post_year is between 2015 and 2020\n",
    "df_filtered = df_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Step 4: Calculate median follower count for each age_group and post_year\n",
    "results = []\n",
    "age_post_years = df_filtered.select(\"age_group\", \"post_year\").distinct().collect()  # Get unique age_group and post_year combinations\n",
    "\n",
    "for row in age_post_years:\n",
    "    age_group = row[\"age_group\"]\n",
    "    post_year = row[\"post_year\"]\n",
    "    \n",
    "    # Filter data for the specific age_group and post_year\n",
    "    df_group = df_filtered.filter((col(\"age_group\") == age_group) & (col(\"post_year\") == post_year))\n",
    "    \n",
    "    # Calculate the median using approxQuantile\n",
    "    median_follower_count = df_group.approxQuantile(\"follower_count\", [0.5], 0.01)[0]\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results.append((age_group, post_year, median_follower_count))\n",
    "\n",
    "# Step 5: Create a new DataFrame for the results\n",
    "schema = StructType([\n",
    "    StructField(\"age_group\", StringType(), True),\n",
    "    StructField(\"post_year\", IntegerType(), True),\n",
    "    StructField(\"median_follower_count\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_median_follower_count = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Order the DataFrame by age_group and post_year\n",
    "df_median_follower_count = df_median_follower_count.orderBy(\"age_group\", \"post_year\")\n",
    "\n",
    "# Show the result\n",
    "df_median_follower_count.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2024-11-15 - S3 Example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
