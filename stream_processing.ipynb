{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b0a858-b314-482f-b41c-48131a444c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "USER_ID = \"0affd9571f39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fea3add-2c06-40e8-addb-30fbd6492102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb2315b-5a15-450c-a1b7-d991a0f5584d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0affd9571f39-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0affd9571f39-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0affd9571f39-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364b4c05-5147-455e-8112-04304bd776d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ind: string (nullable = true)\n |-- unique_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- description: string (nullable = true)\n |-- follower_count: integer (nullable = true)\n |-- poster_name: string (nullable = true)\n |-- tag_list: string (nullable = true)\n |-- is_image_or_video: string (nullable = true)\n |-- image_src: string (nullable = true)\n |-- save_location: string (nullable = true)\n |-- category: string (nullable = true)\n\nroot\n |-- ind: string (nullable = true)\n |-- country: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\nroot\n |-- ind: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- date_joined: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Define schemas for each table\n",
    "schema_pin = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"follower_count\", IntegerType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "schema_geo = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "schema_user = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Decode and parse the JSON data for each stream\n",
    "\n",
    "# Pin Data\n",
    "df_pin_decoded = df_pin.withColumn(\n",
    "    \"decoded_data\", expr(\"CAST(data AS STRING)\")\n",
    ").withColumn(\n",
    "    \"parsed_data\", from_json(col(\"decoded_data\"), schema_pin)\n",
    ").select(\"parsed_data.*\")\n",
    "\n",
    "# Geo Data\n",
    "df_geo_decoded = df_geo.withColumn(\n",
    "    \"decoded_data\", expr(\"CAST(data AS STRING)\")\n",
    ").withColumn(\n",
    "    \"parsed_data\", from_json(col(\"decoded_data\"), schema_geo)\n",
    ").select(\"parsed_data.*\")\n",
    "\n",
    "# User Data\n",
    "df_user_decoded = df_user.withColumn(\n",
    "    \"decoded_data\", expr(\"CAST(data AS STRING)\")\n",
    ").withColumn(\n",
    "    \"parsed_data\", from_json(col(\"decoded_data\"), schema_user)\n",
    ").select(\"parsed_data.*\")\n",
    "\n",
    "# Show the restored columns (for debugging or testing in batch mode)\n",
    "df_pin_decoded.printSchema()\n",
    "df_geo_decoded.printSchema()\n",
    "df_user_decoded.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820b9f6e-1647-481f-b924-169446231b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>data</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "data",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_pin)\n",
    "display(df_geo)\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d03fd1-671e-418a-b176-8316546849dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "from pyspark.sql.functions import col, when, regexp_replace, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1. Replace empty entries and entries with no relevant data with None\n",
    "df_pin_cleaned = df_pin_decoded.replace([\"\", \" \", \"null\", \"N/A\"], None)\n",
    "\n",
    "# 2. Transform `follower_count` to ensure every entry is a number\n",
    "# Handle cases where follower_count contains strings like \"1k\" or \"1M\"\n",
    "def parse_follower_count(follower_count):\n",
    "    if follower_count is None:\n",
    "        return None\n",
    "    if isinstance(follower_count, str):\n",
    "        if \"k\" in follower_count.lower():\n",
    "            return int(float(follower_count.lower().replace(\"k\", \"\")) * 1000)\n",
    "        elif \"m\" in follower_count.lower():\n",
    "            return int(float(follower_count.lower().replace(\"m\", \"\")) * 1000000)\n",
    "    try:\n",
    "        return int(follower_count)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "parse_follower_count_udf = udf(parse_follower_count, IntegerType())\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"follower_count\", parse_follower_count_udf(col(\"follower_count\"))\n",
    ")\n",
    "\n",
    "# 3. Ensure numeric columns have numeric data types\n",
    "numeric_columns = [\"follower_count\"]  # Add other numeric columns if necessary\n",
    "for column in numeric_columns:\n",
    "    df_pin_cleaned = df_pin_cleaned.withColumn(column, col(column).cast(\"int\"))\n",
    "\n",
    "# 4. Clean `save_location` to include only the save location path\n",
    "# Assuming `save_location` has some unwanted prefixes or suffixes to clean\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\n",
    "    \"save_location\", regexp_replace(col(\"save_location\"), r\"unwanted_pattern\", \"\")\n",
    ")\n",
    "\n",
    "# 5. Rename the `index` column to `ind`\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# 6. Reorder columns to the desired order\n",
    "desired_column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\",\n",
    "]\n",
    "\n",
    "df_pin_cleaned = df_pin_cleaned.select(*desired_column_order)\n",
    "\n",
    "from pyspark.sql.functions import array, to_timestamp\n",
    "\n",
    "# 1. Create a new column `coordinates` that contains an array based on the latitude and longitude columns\n",
    "df_geo_cleaned = df_geo_decoded.withColumn(\"coordinates\", array(\"latitude\", \"longitude\"))\n",
    "\n",
    "# 2. Drop the `latitude` and `longitude` columns\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# 3. Convert the `timestamp` column from a string to a timestamp data type\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# 4. Reorder the DataFrame columns to have the desired column order\n",
    "desired_column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "df_geo_cleaned = df_geo_cleaned.select(*desired_column_order)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import concat_ws, to_timestamp\n",
    "\n",
    "# 1. Create a new column `user_name` by concatenating `first_name` and `last_name`\n",
    "df_user_cleaned = df_user_decoded.withColumn(\"user_name\", concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "\n",
    "# 2. Drop the `first_name` and `last_name` columns\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# 3. Convert the `date_joined` column from a string to a timestamp data type\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "\n",
    "# 4. Reorder the DataFrame columns to have the desired column order\n",
    "desired_column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "df_user_cleaned = df_user_cleaned.select(*desired_column_order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d07d0db-2583-471a-bc91-1b82e4fe7353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS pinterest_table;\n",
    "DROP TABLE IF EXISTS geo_table;\n",
    "DROP TABLE IF EXISTS user_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e64160c-1ada-468e-8b4d-1e57f5ca18a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f5568318b20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Pin Data to Delta Table\n",
    "df_pin_cleaned.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/pin/\") \\\n",
    "    .table(\"pinterest_table\")\n",
    "\n",
    "# Write Geo Data to Delta Table\n",
    "df_geo_cleaned.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo/\") \\\n",
    "    .table(\"geo_table\")\n",
    "\n",
    "# Write User Data to Delta Table\n",
    "df_user_cleaned.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/user/\") \\\n",
    "    .table(\"user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1365312452153776,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
